---
title: "Analysis of US Smoking Data (2011-2017)"
author: "Peter J. Ehmann"
date: 'Due: 5/6/2019'
output: html_document
---

```{r setup, include = FALSE}
library(broom)
library(jsonlite)
library(knitr)
library(rvest)
library(tidyverse)
library(xml2)
opts_chunk$set(echo = FALSE)
```

<br>

# Part 1 - Acquire, clean, and format data (export as .csv).

<br>

#### US state smoking data from the Centers for Disease Control (CDC) is availalbe through the Socrata API. The JSON-formatted data for smoking rates by state from 2011-2017 can be accessed from https://chronicdata.cdc.gov/Survey-Data/Table-for-STATE-System-Current-Cigarette-Use-Among/xmxq-jxrr. There are a total of 371 JSON objects (rows) in the original dataset (53 states [includes DC, PR, GU] x 7 rows per state). A few columns of interest (renamed) are shown below. The smoking rate (%) and year are nested as 7x2 tibbles for each state.

```{r smoking_rates, warning = FALSE}
(
smoking_rates <- "https://chronicdata.cdc.gov/resource/gx47-p4ij.json" %>% 
  fromJSON() %>% 
  as_tibble() %>% 
  rename(state = locationdesc, abbr = locationabbr, year = year, percent = data_value) %>% 
  select(state, abbr, year, percent) %>% 
  group_by(state, abbr) %>% 
  nest(.key = smoking_rate) %>% 
  filter(abbr != "PR",
         abbr != "GU")
  )
```

<br>

#### US state tobacco taxation (Cigarette - $ per pack) data from 1995-2019 is also available from the CDC through the Socrata API. Information about the dataset can be obtained at https://chronicdata.cdc.gov/Legislation/CDC-STATE-System-Tobacco-Legislation-Tax/2dwv-vfam. The data can be accessed as JSON and filtered before retrieving it since there are a total of 152,192 rows in the complete dataset. I extracted taxation levels in 2011 and 2017, and calculated a change score for each state.

```{r tax}
(
tax <- paste0("https://chronicdata.cdc.gov/resource/7uzt-fasa.json?", "Year=2011%20AND%20MeasureDesc=%27Cigarette%27%20OR%20Year=2017%20AND%20MeasureDesc=%27Cigarette%27") %>% 
  fromJSON() %>% 
  as_tibble() %>% 
  rename(state = locationdesc, tax_value = provisionvalue) %>% 
  filter(state %in% as_vector(smoking_rates[,1])) %>% 
  mutate(tax_value = as.numeric(tax_value)) %>% 
  select(state, year, tax_value) %>% 
  distinct() %>% 
  group_by(state, year) %>% 
  summarize(value = mean(tax_value)) %>% 
  spread(., year, value) %>% 
  rename(tax_2011 = `2011`, tax_2017 = `2017`) %>% 
  mutate(delta_tax = tax_2017 - tax_2011)
  )
```

<br>

#### US state preemption data from 1995-2019 is also available from the CDC through the Socrata API. Information about the dataset can be obtained at https://chronicdata.cdc.gov/Legislation/CDC-STATE-System-Tobacco-Legislation-Preemption/xsta-sbh5. The data can be accessed as JSON and filtered before retrieving it since there are a total of 107,656 rows in the complete dataset. There are 4 main types of preemptions: (1) xxx, (2) xxx, (3) xxx, and (4) xxx. I extracted a count for each state {Min = 0, Max = 4}.

```{r preemptions}
(
preemptions <- paste0("https://chronicdata.cdc.gov/resource/ksh7-354r.json?", "year=%272017%27%20AND%20provisionaltvalue=%271%27%20OR%20year=%272017%27%20AND%20provisionaltvalue=%272%27%20LIMIT%2050000") %>%
  fromJSON() %>% 
  as_tibble() %>% 
  rename(state = locationdesc, preemption_type = measuredesc) %>% 
  filter(state %in% as_vector(smoking_rates[,1])) %>% 
  mutate(value = as.numeric(provisionaltvalue)) %>% 
  group_by(state, preemption_type) %>% 
  summarize(status = mean(value)) %>% 
  mutate(status = ceiling(status) - 1) %>% 
  select(state, status) %>% 
  group_by(state) %>% 
  summarize(num_of_preemptions = sum(status))
  )
```

<br>

#### US state smoking restrictions law (smokefree indoor air) data from 1995-2019 is also available from the CDC through the Socrata API. Information about the dataset can be obtained at https://chronicdata.cdc.gov/Legislation/CDC-STATE-System-Tobacco-Legislation-Smokefree-Ind/32fd-hyzc. The data can be accessed as JSON and filtered before retrieving it since there are a total of 575,360 rows in the complete dataset. I extracted restrictions in Bars, Restaurants, and Workplaces in 2017 (Q4). Missing values were assumed to be indicative of 'No restriction'.

```{r indoor_air}
(
indoor_air <- paste0("https://chronicdata.cdc.gov/resource/ag3f-urcg.json?", "year=%272017%27%20AND%20measuredesc=%27Private%20Worksites%27%20AND%20provisionaltvalue!=%270%27",  "%20AND%20quarter=%274%27%20AND%20provisionid=%2720%27%20OR%20year=%272017%27%20AND%20", "measuredesc=%27Bars%27%20AND%20provisionaltvalue!=%270%27", "%20AND%20quarter=%274%27%20AND%20provisionid=%27234%27%20OR%20year=%272017%27%20AND%20measuredesc=%27Restaurants%27", "%20AND%20provisionaltvalue!=%270%27%20AND%20quarter=%274%27%20AND%20provisionid=%2737%27") %>% 
  fromJSON() %>% 
  as_tibble %>% 
  rename(state = locationdesc, place = measuredesc, restriction = provisionvalue) %>% 
  filter(state %in% as_vector(smoking_rates[,1])) %>% 
  select(state, place, restriction) %>% 
  spread(., place, restriction) %>% 
  rename(bar_restrictions = `Bars`, work_restrictions = `Private Worksites`, restaurant_restrictions = `Restaurants`) %>% 
  replace_na(list(bar_restrictions = "None", work_restrictions = "None", restaurant_restrictions = "None"))
  )
```

<br>

#### US funding (per capita) data from 1991-2014 is also available from the CDC through the Socrata API. Information about the dataset can be obtained at https://chronicdata.cdc.gov/Funding/University-of-Illinois-at-Chicago-Health-Policy-Ce/vw7y-v3uk. The data can be accessed as JSON and filtered before retrieving it since there are a total of 12,514 rows in the complete dataset. I extracted FUNDING & EXPENDITURES using 'Total Per Capita' in 2014 (most recent data), and calculated the percent of total funding utilized that year.

```{r funding}
(
funding <- paste0("https://chronicdata.cdc.gov/resource/cfhu-b5vt.json?source=%27Total%20Per%20Capita%27%20AND%20year=%272014%27") %>% 
  fromJSON() %>% 
  as_tibble() %>% 
  rename(state = locationdesc, funding = data_value, funding_type = measuredesc) %>% 
  filter(state %in% as_vector(smoking_rates[,1])) %>% 
  select(state, funding, funding_type) %>% 
  group_by(state, funding_type) %>% 
  spread(., funding_type, funding) %>% 
  mutate(expenditures_per_capita = as.numeric(`Expenditures`), 
         funding_per_capita = as.numeric(`Appropriations/Grants`)) %>% 
  select(state, funding_per_capita, expenditures_per_capita) %>% 
  mutate(percent_funding_used = expenditures_per_capita*100/funding_per_capita)
  )
```

<br>

#### US state Medicaid data from 2008-2019 is also available from the CDC through the Socrata API. Information about the dataset can be obtained at https://chronicdata.cdc.gov/Cessation-Coverage-/Medicaid-Coverage-Of-Cessation-Treatments-And-Barr/ntaa-dtex/data. The data can be accessed as JSON and filtered before retrieving it since there are a total of 27,132 rows in the complete dataset. For the last quarter of 2017, I extracted data regarding 'fee for service plans' and 'managed care plans'.

```{r medicaid}
(
medicaid <- paste0("https://chronicdata.cdc.gov/resource/ufr7-5jfh.json?", "year=%272017%27%20AND%20quarter=%274%27%20AND%20measure=%27Medicaid%20Coverage%20of%20Cessation%20Treatments%27") %>% 
  fromJSON() %>% 
  as_tibble() %>% 
  rename(state = locationdesc) %>% 
  filter(state %in% as_vector(smoking_rates[,1])) %>% 
  filter(fee_for_service_plans_altvalue != 3, 
         fee_for_service_plans_altvalue != 4, 
         managed_care_plans_altvalue != 3, 
         managed_care_plans_altvalue != 4, 
         submeasure == "Comprehensive Medicaid Coverage of Treatments") %>% 
  select(state, fee_for_service_plans, managed_care_plans) %>% 
  distinct(state, fee_for_service_plans, managed_care_plans) %>% 
  arrange(state)
  )
```

<br>

#### US state quiline usage (incoming calls per 10,000 state population) data from 2010-2016 is also available from the CDC through the Socrata API. Information about the dataset can be obtained at https://chronicdata.cdc.gov/Quitline/Quitline-Service-Utilization-2010-To-Present/equ4-92qe. The data can be accessed as JSON and filtered before retrieving it since there are a total of 87,240 rows in the complete dataset. I extracted 'number of incoming calls per 10,000 state population' in 2016 (most recent data), and calculated the average over all 4 quarters of that year.

```{r quitline}
(
quitline <- paste0("https://chronicdata.cdc.gov/resource/ew86-dx3i.json?", "variable=%27Incoming%20Calls%20per%2010,000%20State%20Population%27%20AND%20year=%272016%27") %>% 
  fromJSON() %>% 
  as_tibble() %>% 
  rename(state = locationdesc, quitline_percent = value) %>% 
  filter(state %in% as_vector(smoking_rates[,1])) %>% 
  mutate(quitline_percent = as.numeric(quitline_percent)) %>% 
  select(state, quitline_percent) %>% 
  group_by(state) %>% 
  summarise(quitline_calls_per_10000 = mean(quitline_percent))
  )
```

<br>

#### Information about US state region classifications can be scraped from the main list in the HTML of https://simple.wikipedia.org/wiki/List_of_regions_of_the_United_States. The div, ul, and il tags can be used to extract each state's region and division (sub-region).

```{r scrape_wiki_html}
(
states_html <- read_html("https://simple.wikipedia.org/wiki/List_of_regions_of_the_United_States")
  )

states <- states_html %>% 
  html_nodes(xpath = "//div/ul/li/ul/li/ul/li") %>% 
  html_text()

divisions <- states_html %>% 
  html_nodes(xpath = "//div/div/div/div/ul/li/ul/li") %>% 
  html_text()

regions <- states_html %>% 
  html_nodes(xpath = "//div/div/div/div/ul/li") %>% 
  html_text()
```

```{r extract_division}
division_names <- divisions %>% 
  str_extract(., "\\((.*)\\)") %>% 
  str_extract(., "[\\w+\\s?]+")

division_counts <- divisions %>% 
  str_count(., "\n")

division_list = ""
for (i in 1:length(division_names)) {
  division_list <- c(division_list, (rep(division_names[i], division_counts[i])))
}
```

```{r extract_region}
region_names <- regions %>% 
  .[1:4] %>% 
  str_extract(., "\\((.*)\\)") %>% 
  str_extract(., "[\\w+\\s?]+")

region_counts <- regions %>% 
  .[1:4] %>% 
  str_count(., "\n")

divisions_per_region <- regions %>% 
  .[1:4] %>% 
  str_count(., "Division")

region_list = ""
for (i in 1:length(region_names)) {
  region_list <- c(region_list, (rep(region_names[i], region_counts[i]-divisions_per_region[i])))
}
```

```{r state_table}
state_table <- as_tibble(data.frame(matrix(nrow = length(states), ncol = 3)))
colnames(state_table) <- c("state", "region", "division")
state_table[,1] <- states
state_table[,2] <- region_list[-1]
state_table[,3] <- division_list[-1]
state_table %>% arrange(state)
```

<br>

#### The datasets are merged by STATE and the nested tibbles are unnested.

```{r merge_data}
df_clean <- merge(smoking_rates, tax, by = "state") %>% 
  merge(., preemptions, by = "state") %>% 
  merge(., indoor_air, by = "state", all = TRUE) %>% 
  merge(., funding, by = "state") %>% 
  merge(., medicaid, by = "state") %>% 
  merge(., quitline, by = "state") %>% 
  merge(., state_table, by = "state") %>% 
  replace_na(list(bar_restrictions = "None", work_restrictions = "None", restaurant_restrictions = "None")) %>% 
  unnest()
```

<br>

#### I also extracted simple linear regression parameters (b0 = intercept @ t=0 [2011], b1 = slope) at three different levels (state, division, region) and added them to the dataset.

```{r fxn_extract_regression_int_slope}
regression <- function(df) {
  lm(percent ~ year, data = df)
}
parameter <- function(df, choice) {
  tidy(df)[[choice,"estimate"]]
}
```

```{r state_regression}
df_clean <- df_clean %>% 
  mutate(year = as.numeric(year) - 2011) %>% 
  group_by(state) %>% 
  nest() %>% 
  mutate(lm_fit = map(data, regression)) %>% 
  mutate(state_int = as.numeric(map2(lm_fit, 1, parameter))) %>% 
  mutate(state_slope = as.numeric(map2(lm_fit, 2, parameter))) %>% 
  select(-lm_fit) %>% 
  unnest() %>% 
  mutate(year = year + 2011)
```

```{r division_regression}
df_clean <- df_clean %>% 
  mutate(year = year - 2011) %>% 
  group_by(division) %>% 
  nest() %>% 
  mutate(lm_fit = map(data, regression)) %>% 
  mutate(division_int = as.numeric(map2(lm_fit, 1, parameter))) %>% 
  mutate(division_slope = as.numeric(map2(lm_fit, 2, parameter))) %>% 
  select(-lm_fit) %>% 
  unnest() %>% 
  mutate(year = year + 2011)
```

```{r region_regression}
df_clean <- df_clean %>% 
  mutate(year = year - 2011) %>% 
  group_by(region) %>% 
  nest() %>% 
  mutate(lm_fit = map(data, regression)) %>% 
  mutate(region_int = as.numeric(map2(lm_fit, 1, parameter))) %>% 
  mutate(region_slope = as.numeric(map2(lm_fit, 2, parameter))) %>% 
  select(-lm_fit) %>% 
  unnest() %>% 
  mutate(year = year + 2011)
```

<br>

#### The last step is to export the data as a .csv file which is posted in the GitHub repository.

```{r write_csv}
# uncomment line 306 to clear the global enviroment of all variables
df_clean %>% select(state, abbr, region, division, year, percent, state_int, state_slope, division_int, division_slope, 
                    region_int, region_slope, tax_2011, tax_2017, delta_tax, num_of_preemptions, bar_restrictions, 
                    work_restrictions, restaurant_restrictions, funding_per_capita, expenditures_per_capita, percent_funding_used, 
                    fee_for_service_plans, managed_care_plans, quitline_calls_per_10000) %>% 
  write_csv("smoking_data.csv")
# rm(list = ls(all = TRUE))
```

<br>

# Part 2 - Data visualization and statistics.

<br>

#### The cleaned data can be retreived from the GitHub repository at https://raw.githubusercontent.com/peter-ehmann/Data_Wrangling_Project_S2019/master/smoking_data.csv.

```{r import_clean_data, message = FALSE}
(
data <- read_csv("https://raw.githubusercontent.com/peter-ehmann/Data_Wrangling_Project_S2019/master/smoking_data.csv")
  )
```

<br>

### First, we will look at differences in smoking laws and policies by region.

<br>

#### TAXATION. From the bar plot, it is apparent that Northeast states tax cigarettes more and had the greatest increase from 2011-2017. The South had the lowest tax rates and also the smallest increase in tax from 2011-2017.

```{r tax_analysis}
data %>% distinct(state, region, tax_2011, tax_2017, delta_tax) %>% 
  group_by(region) %>% 
  summarize(tax_2011_avg = mean(tax_2011), 
            tax_2017_avg = mean(tax_2017), 
            delta_tax_avg = mean(delta_tax)) %>% 
  gather(., variable, value, -region) %>% 
  ggplot(., mapping = aes(x = variable, y = value, fill = region)) + 
    geom_col(position = position_dodge()) + 
    xlab("") + 
    ylab("Tax per carton of Cigarettes ($)") + 
    ggtitle("Taxation of Cigarettes by US Region") + 
    scale_x_discrete(labels = c("delta_tax_avg" = "Change", "tax_2011_avg" = "2011", "tax_2017_avg" = "2017"))
```

<br>

#### PREEMPTIONS. The bar graph shows wide variability in the number of states with preemptions within each region.

```{r preemption_analysis}
data %>% distinct(state, region, num_of_preemptions) %>% 
  group_by(region, num_of_preemptions) %>% 
  tally() %>% 
  spread(., num_of_preemptions, n) %>% 
  replace_na(list(`0` = 0, `1` = 0, `2` = 0, `3` = 0, `4` = 0)) %>% 
  gather(., num_pre, num_states, -region) %>% 
  ggplot(., mapping = aes(x = num_pre, y = num_states, fill = region)) + 
    geom_col() + 
    xlab("Number of Preemptions") + 
    ylab("Number of States") + 
    ggtitle("Number of Preemptions by Region (Max = 4)")
```

<br>

#### INDOOR AIR. The South appaers to have the most instances of 'None' across bars, restaurants, and workplaces.

```{r indoor_analysis}
indoor_air_analysis <- data %>% 
  distinct(state, region, bar_restrictions, work_restrictions, restaurant_restrictions) %>% 
  group_by(region, bar_restrictions, work_restrictions, restaurant_restrictions) %>% 
  tally() %>% 
  spread(., bar_restrictions, n) %>% 
  replace_na(list(`Banned` = 0, `Designated Areas` = 0, `None` = 0, `Separate Ventilated Areas` = 0)) %>% 
  gather(., bar_restrictions, n, `Banned`:`Separate Ventilated Areas`) %>% 
  spread(., work_restrictions, n) %>% 
  replace_na(list(`Banned` = 0, `Designated Areas` = 0, `None` = 0, `Separate Ventilated Areas` = 0)) %>% 
  gather(., work_restrictions, n, `Banned`:`Separate Ventilated Areas`) %>% 
  spread(., restaurant_restrictions, n) %>% 
  replace_na(list(`Banned` = 0, `Designated Areas` = 0, `None` = 0, `Separate Ventilated Areas` = 0)) %>% 
  gather(., restaurant_restrictions, n, `Banned`:`Separate Ventilated Areas`)

ggplot(indoor_air_analysis, mapping = aes(x = bar_restrictions, y = n, fill = region)) + 
  geom_col() + 
  xlab("") + 
  ylab("Number of States") + 
  ggtitle("Bar Restrictions")

ggplot(indoor_air_analysis, mapping = aes(x = work_restrictions, y = n, fill = region)) + 
  geom_col() + 
  xlab("") + 
  ylab("Number of States") + 
  ggtitle("Work Restrictions")

ggplot(indoor_air_analysis, mapping = aes(x = restaurant_restrictions, y = n, fill = region)) + 
  geom_col() + 
  xlab("") + 
  ylab("Number of States") + 
  ggtitle("Restaurant Restrictions")
```

<br>

#### FUNDING. At least half of Northeast and Midwest states use all the money provided through various funding sources.

```{r funding_analysis}
data %>% distinct(state, region, percent_funding_used) %>% 
  group_by(region) %>% 
  add_tally(percent_funding_used > 99.99) %>% 
  add_tally(n()) %>% 
  mutate(percent = n*100/nn) %>% 
  distinct(region, percent) %>% 
  ggplot(., mapping = aes(x = "", y = percent, fill = region)) + 
    geom_col(position = position_dodge()) + 
    xlab("") + 
    ylab("Percent of States in Region") + 
    ggtitle("States Expending >99.99 % of Funding Provided")
```

<br>

#### MEDICAID.

```{r medicaid_analysis}
medicaid_analysis <- data %>% 
  distinct(state, region, fee_for_service_plans, managed_care_plans) %>% 
  group_by(region, fee_for_service_plans, managed_care_plans) %>% 
  tally() %>% 
  spread(., fee_for_service_plans, n) %>% 
  replace_na(list(`No` = 0, `Not Applicable` = 0, `Yes` = 0)) %>% 
  gather(., fee_for_service_plans, n, `No`:`Yes`) %>% 
  spread(., managed_care_plans, n) %>% 
  replace_na(list(`No` = 0, `Not Applicable` = 0, `Yes` = 0)) %>% 
  gather(., managed_care_plans, n, `No`:`Yes`)

ggplot(medicaid_analysis, mapping = aes(x = fee_for_service_plans, y = n, fill = region)) + 
  geom_col() + 
  xlab("") + 
  ylab("Number of States") + 
  ggtitle("Fee for Service Plans")

ggplot(medicaid_analysis, mapping = aes(x = managed_care_plans, y = n, fill = region)) + 
  geom_col() + 
  xlab("") + 
  ylab("Number of States") + 
  ggtitle("Managed Care Plans")
```

<br>

#### QUITLINE. The higher amount of quitline users may be highly correlated with number of smokers. This is confirmed by the linear regression analysis below.

```{r quitline_analysis}
data %>% distinct(state, region, quitline_calls_per_10000) %>% 
  group_by(region) %>% 
  summarize(mean = mean(quitline_calls_per_10000)) %>% 
  ggplot(., mapping = aes(x = "", y = mean, fill = region)) + 
    geom_bar(width = 1, stat = "identity") + 
    coord_polar("y", start = 0) + 
    theme(axis.text.x = element_blank()) + 
    xlab("") + 
    ylab("") + 
    ggtitle("Percent of Quitline Calls by Region")

summary(
  lm(quitline_calls_per_10000 ~ state_int, data = data %>% distinct(state, state_int, quitline_calls_per_10000))
)
```

<br>

### Now, we will plot out smoking rates from 2011-2017.

<br>

#### A simple line plot grouped by REGION shows a similar linear decrease in '% smokers' for all 4 US regions. However, the MIDWEST/SOUTH have much higher smoking rate for all years than the NORTHEAST/WEST.

```{r region_line_plot}
region_means <- data %>% 
  select(state, region, year, percent) %>% 
  group_by(region, year) %>% 
  summarize(average = mean(percent))

ggplot(region_means, mapping = aes(x = year, y = average, color = region, group = region)) + 
  geom_point() + 
  geom_line() + 
  xlab("Year") + 
  ylab("Current Cigarette Smokers (%)") + 
  ggtitle("Smoking Rates (%) by US Region") + 
  scale_x_continuous(breaks = pretty(region_means$year, n = 7))
```

<br>

#### The table below shows approximate smoking rates in 2011 (intercept) and rate of change from 2011-2017 (slope) by REGION (4).

```{r region_int_slope}
data %>% distinct(region, region_int, region_slope)
```

<br>

#### A simple line plot grouped by DIVISION (sub-region) shows more variability than when grouped by REGION.

```{r division_line_plot}
division_means <- data %>% 
  select(state, division, year, percent) %>% 
  group_by(division, year) %>% 
  summarize(average = mean(percent))

ggplot(division_means, mapping = aes(x = year, y = average, color = division, group = division)) + 
  geom_point() + 
  geom_line() + 
  xlab("Year") + 
  ylab("Current Cigarette Smokers (%)") + 
  ggtitle("Smoking Rates (%) by US Division") + 
  scale_x_continuous(breaks = pretty(division_means$year, n = 7))
```

<br>

#### The table below shows approximate smoking rates in 2011 (intercept) and rate of change from 2011-2017 (slope) by DIVISION (9).

```{r division_int_slope}
data %>% distinct(division, division_int, division_slope)
```

<br>

#### You can see even more variability in each state's smoking trends over 2011-2017.

```{r state_line_plot}
ggplot(data, mapping = aes(x = year, y = percent, color = state, group = state)) + 
  geom_point() + 
  geom_line() + 
  xlab("Year") + 
  ylab("Current Cigarette Smokers (%)") + 
  ggtitle("Smoking Rates (%) by State") + 
  scale_x_continuous(breaks = pretty(data$year, n = 7)) + 
  theme(legend.position = "none")
```

<br>

#### Top 5 STATE intercepts (highest smoking rates in 2011).

```{r state_int_high}
data %>% distinct(state, state_int) %>% 
  arrange(desc(state_int)) %>% 
  head()
```

<br>

#### Bottom 4 STATE intercepts (lowest smoking rates in 2011).

```{r state_int_low}
data %>% distinct(state, state_int) %>% 
  arrange(state_int) %>% 
  head()
```

<br>

#### Largest (most negative) STATE slopes (greatest rate of change in reducing smokers 2011-2017).

```{r state_slope_high}
data %>% distinct(state, state_slope) %>% 
  arrange(state_slope) %>% 
  head()
```

<br>

#### Smallest (least negative) STATE slopes (smallest rate of change in reducing smokers 2011-2017).

```{r state_slope_low}
data %>% distinct(state, state_slope) %>% 
  arrange(desc(state_slope)) %>% 
  head()
```

<br>

#### Individual state trajectories (slope) may be influenced by where they start (intercept). Here, is a regression model showing a nonsignificant (p>0.05) negative relationship between slope and intercept for each state. The plot generated below suggests a trend such that a greater intercept results in lower [more negative] slope. This is confirmed by the negative 'state_int' estimate in the model summary.

```{r corr_int_slope}
summary(
  lm(state_slope ~ state_int, data = data %>% distinct(state, state_int, state_slope))
)

data %>% distinct(state, state_int, state_slope) %>% 
  ggplot(., mapping = aes(x = state_int, y = state_slope)) + 
    geom_point() + 
    stat_smooth(method = "lm") +
    xlab("Intercept (b0)") + 
    ylab("Slope (b1)") +
    ggtitle("Correlation between state intercept and slope coefficients")
```

<br>

### To wrap up, we will investigate factors that influence STATE intercept and slope coefficients.

<br>

ANALYSIS OF FACTORS INFLUENCING INT AND SLOPE

<br>

#### Information about the packages used, their versions, and the version of R that was used.

```{r session_info}
devtools::session_info()
```
